# Exercise1

If we adopt $\star \alpha(X) = \alpha(JX)$, then

$$
\begin{aligned}
\star \alpha \wedge \alpha (X, JX) &= \star \alpha(X) \alpha(JX) - \star \alpha(JX) \alpha(X) \\
                                   &= \alpha(JX) \alpha(JX) - \alpha(JJX) \alpha(X) \\
                                   &= \alpha(JX) \alpha(JX) + \alpha(X) \alpha(X) \\
                                   &\geq 0
\end{aligned}
$$

Since any vector $Y$ on the tangent space can be decomposed to a linear combinition of $X$ and $JX$, $\star \alpha \wedge \alpha (X, Y) \geq 0$ always holds. Therefore, $\langle \langle \alpha, \beta \rangle \rangle = \int_M \star \alpha \wedge \beta$ is positive-definite.

Similarly, if we adopt $\star \alpha(X) = -\alpha(JX)$, then

$$
\begin{aligned}
\alpha \wedge \star \alpha (X, JX) &= \alpha(X) \star \alpha(JX) - \alpha(JX) \star \alpha(X) \\
                                   &= -\alpha(X) \alpha(JJX) + \alpha(JX) \alpha(JX) \\
                                   &= \alpha(X) \alpha(X) + \alpha(JX) \alpha(JX) \\
                                   &\geq 0
\end{aligned}
$$

which means $\langle \langle \alpha, \beta \rangle \rangle = \int_M \alpha \wedge \star \beta$ is positive-definite.

# Exercise2

$$
\begin{aligned}
(\star \star \alpha) \wedge (\star \alpha) (X, JX) &= \star \star \alpha(X) \star \alpha(JX) - \star \star \alpha(JX) \star \alpha(X) \\
                                         &= \star \alpha(JX) \star \alpha(JX) + \star \alpha(X) \star \alpha(X) \\
                                         &= \alpha(JX) \alpha(JX) + \alpha(X) \alpha(X) \\
                                         &= \star \alpha \wedge \alpha (X, JX)
\end{aligned}
$$

Therefore, 

$$
\begin{aligned}
|| \star \alpha || &= \sqrt{\langle \langle \star \alpha, \star \alpha \rangle \rangle} =\sqrt{\int_M (\star \star \alpha) \wedge (\star \alpha)} \\
                   &= \sqrt{\int_M \star \alpha \wedge \alpha} \\
                   &= \sqrt{\langle \langle \alpha, \alpha \rangle \rangle} = || \alpha ||
\end{aligned}
$$

which implies rotation does not change length.

# Exercise3

Let $u = a + b i$ and $v = c + d i$, then

$$
\begin{aligned}
\bar{u} v &= (a - b i) (c + d i) \\
          &= (ac + bd) + (ad - bc) i \\
          &= u \cdot v + (u \times v) i
\end{aligned}
$$

# Exercise4

$$
\begin{aligned}
\langle u, u \rangle = \bar{u} u &= u \cdot u + (u \times u) i = u \cdot u \geq 0
\end{aligned}
$$

$$
\begin{aligned}
\langle v, u \rangle = \bar{v} u &= v \cdot u + (v \times u) i = u \cdot v - (u \times v) i = \overline{\langle u, v \rangle}
\end{aligned}
$$

Therefore, the inner product is Hermitian and positive-definite.

# Exercise5

$$
\begin{aligned}
\star \bar{\alpha} \wedge \alpha (X, JX) &= \star \bar{\alpha}(X) \alpha(JX) - \star \bar{\alpha}(JX) \alpha(X) \\
                                         &= \bar{\alpha}(JX) \alpha(JX) + \bar{\alpha}(X) \alpha(X) \\
                                         &= \langle \alpha(JX), \alpha(JX) \rangle + \langle \alpha(X), \alpha(X) \rangle \\
                                         &\geq 0
\end{aligned}
$$

$$
\begin{aligned}
\star \bar{\alpha} \wedge \beta (X, JX) &= \star \bar{\alpha}(X) \beta(JX) - \star \bar{\alpha}(JX) \beta(X) \\
                                        &= \bar{\alpha}(JX) \beta(JX) + \bar{\alpha}(X) \beta(X) \\
                                        &= \langle \alpha(JX), \beta(JX) \rangle + \langle \alpha(X), \beta(X) \rangle \\
\end{aligned}
$$

$$
\begin{aligned}
\star \bar{\beta} \wedge \alpha (X, JX) &= \langle \beta(JX), \alpha(JX) \rangle + \langle \beta(X), \alpha(X) \rangle \\
                                        &= \overline{\langle \alpha(JX), \beta(JX) \rangle} + \overline{\langle \alpha(X), \beta(X) \rangle} \\
                                        &= \overline{\star \bar{\alpha} \wedge \beta (X, JX)}
\end{aligned}
$$

Since any vector $Y$ on the tangent space can be decomposed to a linear combinition of $X$ and $JX$, $\langle \langle \alpha, \beta \rangle \rangle = \text{Re} \int_M \star \bar{\alpha} \wedge \beta$ is Hermitian and positive-definite.

# Exercise6

According to Stoke's theorem,

$$
\begin{aligned}
\text{Re} \int_\partial v \star \overline{du} &= \text{Re} \int d(v \star \overline{du}) = \text{Re} \int dv \wedge \star \overline{du} + \text{Re} \int v \wedge d \star \overline{du} \\
                         &= \text{Re} \int dv \wedge \star \overline{du} + \text{Re} \int v \wedge d \star \overline{du}
\end{aligned}
$$

Since the normal derivative of either function vanishes along the boundary,

$$
\begin{aligned}
\text{Re} \int_\partial v \star \overline{du} &= \text{Re} \int dv \wedge \star \overline{du} + \text{Re} \int v \wedge d \star \overline{du} = 0
\end{aligned}
$$

$$
\begin{aligned}
\text{Re} \int dv \wedge \star \overline{du} &= -\text{Re} \int v \wedge d \star \overline{du} = -\text{Re} \int v \wedge \star \star d \star d \overline{u} \\
&= \text{Re} \int v \wedge \star \Delta \overline{u}
\end{aligned}
$$

Therefore,

$$
\begin{aligned}
\langle \langle du, dv \rangle \rangle = \langle \langle \Delta u, v \rangle \rangle
\end{aligned}
$$

# Exercise7

$$
\begin{aligned}
dA &= (-i \ dz) \times (dz) \\
   &= -\frac{i}{2} d\bar{z} \wedge dz
\end{aligned}
$$

Therefore,

$$
\begin{aligned}
A(z) = -\frac{i}{2} \int_M d\bar{z} \wedge dz
\end{aligned}
$$

# Exercise8

$$
\begin{aligned}
E_C(z) &= \frac{1}{4} || \star dz - i \ dz ||^2 = \frac{1}{4} \langle \langle  \star dz - i \ dz,  \star dz - i \ dz \rangle \rangle \\
       &= \frac{1}{4} \Big( \langle \langle \star dz, \star dz \rangle \rangle + \langle \langle \star dz, i \ dz \rangle \rangle + \langle \langle i \ dz, \star dz \rangle \rangle + \langle \langle  i \ dz, i \ dz \rangle \rangle \Big) \\
       &= \frac{1}{2} \langle \langle  dz, dz \rangle \rangle + \frac{i}{2} \langle \langle \star dz, dz \rangle \rangle \\
       &= \frac{1}{2} \langle \langle  \Delta z, z \rangle \rangle + \frac{i}{2} \int_M \star \star d \bar{z} \wedge d z\\
       &= E_D(z) - A(z)
\end{aligned}
$$

# Exercise9

According to Stoke's theorem,

$$
\begin{aligned}
\int_M d \bar{z} \wedge dz &= \int_{\partial M} \bar{z} dz \\
                           &= \sum_{e_{ij} \in \partial M} (z_j - z_i) \int_{e_{ij}} \bar{z} \\
                           &= \sum_{e_{ij} \in \partial M} \frac{1}{2} (z_j - z_i)(\bar{z}_i + \bar{z}_j) \\
                           &= \frac{1}{2} \sum_{e_{ij} \in \partial M} \bar{z}_i z_j - \bar{z}_j z_i
\end{aligned}
$$

Therefore,

$$
\begin{aligned}
A(z) = \frac{i}{4} \sum_{e_{ij} \in \partial M} \bar{z}_i z_j - \bar{z}_j z_i
\end{aligned}
$$

# Exercise10

A holomorphic map $z$ satisfies the Cauchy-Riemann equation:

$$
\begin{aligned}
\star dz = i \ dz
\end{aligned}
$$

Then,

$$
\begin{aligned}
d \star dz = d \ (i \ dz) = 0
\end{aligned}
$$

$$
\begin{aligned}
\Delta z = - \star d \star d z = 0
\end{aligned}
$$

which means $z$ sits in the kernel of the Laplace-Beltrami operator $\Delta$.

# Exercise11

$\Delta \phi = 0$ means the curvature is 0 everywhere, or there's no bending on the mesh. Suppose $\phi (M)$ sits on the plane, then the laplacian equation already satisfies. There's no guarantee that $\phi$ will preserve angles.

# Exercise12

Suppose $x$ is an eigen-vector of $A$, then

$$
\begin{aligned}
\langle \langle A x, x \rangle \rangle = \langle \langle \lambda x, x \rangle \rangle = \lambda \langle \langle x, x \rangle \rangle
\end{aligned}
$$

$$
\begin{aligned}
\langle \langle A x, x \rangle \rangle = \langle \langle x, A^* x \rangle \rangle = \langle \langle x, A x \rangle \rangle = \langle \langle x, \lambda x \rangle \rangle = \bar{\lambda} \langle \langle x, x \rangle \rangle
\end{aligned}
$$

Therefore,

$$
\begin{aligned}
\lambda = \bar{\lambda}
\end{aligned}
$$

which means that all the eigenvalues of $A$ are real.

# Exercise13

$$
\begin{aligned}
\langle \langle A e_i, e_j \rangle \rangle &= \langle \langle \lambda_i e_i, e_j \rangle \rangle = \lambda_i \langle \langle e_i, e_j \rangle \rangle \\
&= \langle \langle e_i, A e_j \rangle \rangle = \langle \langle e_i, \lambda_j e_j \rangle \rangle = \lambda_j \langle \langle e_i, e_j \rangle \rangle
\end{aligned}
$$

Since $\lambda_i$ and $\lambda_j$ are 2 different eigen values, there must be

$$
\begin{aligned}
\langle \langle e_i, e_j \rangle \rangle = 0
\end{aligned}
$$

# Exercise14

Let

$$
\begin{aligned}
L = x^T A x - \lambda (x^T x - 1)
\end{aligned}
$$

Then,

$$
\begin{aligned}
\frac{\partial L}{\partial x} = 2A x - 2 \lambda x = 0 \implies A x = \lambda x
\end{aligned}
$$

$$
\begin{aligned}
\frac{\partial L}{\partial \lambda} = x^T x - 1 = 0 \implies x^T x = 1
\end{aligned}
$$

which means $\lambda$ is an eigen value of A, and $x$ is the corresponding eigen vector. Plugging in $\lambda$ and $x$, we have

$$
\begin{aligned}
x^T A x = x^T (\lambda x) = \lambda
\end{aligned}
$$

Therefore, $x^T A x$ takes the minimum when $\lambda$ is the smallest eigen value and $x$ is the corresponding eigen vector.

# Exercise15

See [Power Iteration Analysis](https://en.wikipedia.org/wiki/Power_iteration#Analysis).

# Exercise16

$$
\begin{aligned}
A e = \lambda e
\end{aligned}
$$

Since $A$ is invertible,

$$
\begin{aligned}
e = \lambda A^{-1} e \implies \frac{1}{\lambda} e = A^{-1} e
\end{aligned}
$$

which means $e$ is also an eigenfunction of the inverse with the reciprocal eigenvalue.

From geometry perspective, it means that we can retrieve the eigenvectors by "squishing" back the axis.